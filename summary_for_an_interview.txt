======================Типы и работа моделей======================
Метрические алгоритмы используют расстояния между объектами, для них вводится понятие метрики: Евклидовой, Манхэттенской, Минковского) или функции близости (меры похожести, те значение тем больше, чем больше схожесть объектов): скалярное произведенеие, косинусная мера (в 
    задачах анализа текстов), коэффициент корреляции (в рекомендательных системах)
    - Метод ближайших соседей (kNN). Используются расстояния между векторами и подобранные веса.
    - Центроидный классификатор. Сначала находятся центра, а потом каждый x относится к ближайшему центру в пространстве признаков
    - Взвешенный kNN для регрессии 

======================Теоретические вопросы======================
Предобработка данных:
    Обучение с учителем производится на размеченных данных, которые представляют собой выборку из объектов с их 
    признаковым описанием для которых известны ответы
    Чтобы работать с той или иной моделью нужно:
        • Выбрать функционал (функцию) ошибки, то есть задать способ определения качества работы того или
        иного алгоритма на обучающей выборке.
        • Построить семейство алгоритмов, то есть множество алгоритмов, из которого потом будет выбираться
        наилучший с точки зрения определенного функционала ошибки.
        • Ввести метод обучения, то есть определить способ выбора лучшего алгоритма из семейства.
    - Обязательно проводить масштабирование признаков. Дело в том, что градиентный спуск работает хорошо, только 
    когда линии уровня похожи на круги, а это происходит когда они координаты векторов удалены друг от друга на 
    соразмерное расстояние. В противном случае линии уровня начинают напоминать элипсы, а вектор градиента будет 
    проходить точки мимо минимума функции.
    - Всегда перемешивать выборку, кроме некоторых случаев вроде предсказания будущего.
    - Для работы с категориальными признаками их нужно преобразовать. Это могут быть бинарное преобразование, где каждый признак заменяется 
    n-признаками, где n - количество его возможных значений.
    - Если для задачи классификации выборка несбалансированна, то можно использовать undersampling или oversampling.
    - При обучением кросс-валидацией на несбалансированной выборке может возникнуть ситуация, что на некоторых блоках в обучающую выборку 
    могут не попасть объекты из одного или больше классов. Таким образом на выходе этих блоков получится классификатор, который никогда не 
    видел этих классов. Чтобы с этим бороться необходимо использовать стратификацию, то есть делать так, чтобы распределение классов в 
    каждом блоке примерно совпадало с распределением классов в исходной выборке.
--
Постановка задачи классификации:
    Центральная задача - интерполяция — задача восстановления функции по нескольким точкам, в которых известны ее 
    значения.
    Отнести каждый из объектов выборки к одному из классов используя его признаковое описание. Может быть как 
    бинарная, так и многоклассовая.
    Примеры бинарной классификации:
        • Понравится ли пользователю фильм?
        • Вернет ли клиент кредит?
    Многоклассовой:
        • Из какого сорта винограда сделано вино?
        • Какая тема статьи?
        • Машина какого типа изображена на фотографии: мотоцикл, легковая или грузовая машина?
--
Постановка задачи регрессии:
    Когда y является вещественной переменной, говорят о задаче регрессии. Задачей является предсказание 
    некоторого вещественного числа по некоторому признаковому описанию объекта. Например:
        • Предсказание температуры на завтра.
        • Прогнозирование выручки магазина за год.
        • Оценка возраста человека по его фото.

--
Метрики качества в задачах классификации и регрессии:
    Метрики качества могут использоваться:
        • Для задания функционала ошибки (используется при обучении).
        • Для подбора гиперпараметров (используется при измерении качества на кросс-валидации). В том числе
        можно использовать другую метрику, которая отличается от метрики, с помощью которой построен
        функционал ошибки.
        • Для оценивания итоговой модели: пригодна ли модель для решения задачи
    Для регрессии:
    — Среднеквадратичная ошибка. Этот функционал сильно штрафует за большие ошибки, так как отклонения возводятся 
    в квадрат. Это приводит к тому, что штраф на выбросе будет очень сильным, и алгоритм будет настраиваться на 
    выбросы.
    - Средняя абсолютная ошибка. Этот функционал сложнее минимизировать, так как у модуля производная не 
    существует в нуле. Но у такого функционала больше устойчивость к выбросам, так как штраф за сильное 
    отклонение гораздо меньше.
    - Коэффициент детерминации R2. Позволяет интерпретировать значение среднеквадратичной ошибки. Этот 
    коэффициент показывает, какую долю дисперсии (разнообразия ответов) во всем целевом векторе y модель смогла 
    объяснить.
    - Квантильная ошибка. Подходит если нужно штрафовать больше за какой-то один вид ошибки.
    Для классификации:
    - Доля правильных ответов. Эта метрика имеет некоторые серьезные недостатки.
    Первая проблема связана с несбалансированными выборками. Показателен следующий пример. Пусть в выборке 1000 
    объектов, из которых 950 относятся к классу −1 и 50 — к классу +1. Рассматривается бесполезный
    (поскольку не восстанавливает никаких закономерностей в данных) константный классификатор, который
    на всех объектах возвращает ответ −1. Но доля правильных ответов на этих данных будет равна 0.95, что
    несколько много для бесполезного классификатора.
    Вторая проблема с долей верных ответов состоит в том, что она никак не учитывает разные цены разных
    типов ошибок. Пример, когда цены разные - кредитный скоринг.
    - Точность и полнота. Вводятся два вида ошибок: False positive (ложные срабатывания) и False negative (ложные 
    пропуски). Тогда точность: TP/TP+FP. Полнота: TP/TP+FN.
    Первая показывает насколько можно доверять классификатору в случае срабатывания, а вторая на какой доле 
    истинных ответов алгоритм срабатывает.
    - F-мера. Гармоническое среднее. 2*P*R/P+R. Работает как минимум, но при этом коррелирует с максимальным 
    значением. Можно так же использовать расширенную F-меру которая предусматривает регуляризацию  в пользу 
    полноты или точности добавлением коэффициента бета.
    Часто бывает необходимо оценить качество именно оценки принадлежности (вероятности), а порог выбирается позже 
    из соображений на точность или полноту:
    - AUC-PRC. Площадь под PR-кривой. строится график в осях precision-recall, затем откладываются точки, 
    каждой из которых соответсвует классификатор с некоторым порогом (от большего к меньшему). Кривая 
    начинается из точки 0,0 и заканчивается в точке 1,r где r - доля объектов класса 1. Площадь под этой 
    кривой является хорошой мерой оценки качества классификатора. Ось precision зависит от количества 
    объектов того или иного типа в выборке.
    - AUC-ROC. Строится график в осях FPR и TPR. FPR=FP/FP+TN, TPR=TP/TP+FN. Точки соответствуют различным 
    порогам классификатора. Площадь под кривой характеризует качество отнесения к первому классу. AUC-ROC 
    нормируется на размеры классов и не зависит от баланса классов.     
--
Понятие функции ошибки:
    Используется для настройки модели (к примеру с помощью стохастического градиентного спуска). В ходе обучения 
    модели минимизируется.
    Принимает на вход ответ алгоритма и правильный ответ
    Характеристика качества работы алгоритма. Способ измерения того, хорошо или плохо работает алгоритм на 
    конкретной выборке. Результат функции ошибки один из важных показателей для выбора оптимального алгоритма из 
    множества алгоритмов
    Виды функций ошибки: модуль отклонения (недефиринцируемая), квадрат отклонения, доля неправильных ответов для 
    бин. кл. - пороговая недифферинциуемая. Для оценки пороговой функции потерь используются гладкие функции: 
    логистическая функция, экспоненциальная функция, кусочно-линейная функция метода опроных векторов.
--
Машинное обучение как минимизация функционала ошибки:  
    Задача машинного обучения это задача восстановления функции по нескольким точкам, в которых известны ее 
    значения. Центральным понятием машинного обучения является обучающая выборка. Это те примеры, на основе 
    которых будет строиться общая закономерность. Предсказание делается на основе некоторой модели (алгоритма), 
    которая представляет из себя функцию и пространства X в пространство Y. Не все алгоритмы подходят для решения 
    задачи, поэтому вводится понятие функционала ошибки. Задача обучения состоит в подборе такого алгоритма, для 
    которого достигается минимум функционала ошибки. Лучший в этом смысле алогритм a выбирается из некоторого 
    семейства А. Минимизация ошибки может происходит за счет оптимизации модели: вектора весов в линейной модели, перебором подмножества признаков при их отборе,  
--
Линейные модели:
    Модели на основе линейных уравнений вида sum(wi*xi)+w0 - признаки умноженные на веса плюс свободный 
    коэффициент. В ходе обучения подбираются такие веса, что бы как можно сильнее уменьшить функционал ошибки на 
    обучающей выборке.
    Чем больше признаков, тем более сложные взаимосвязи может восстанавливать модель, но тем больше вероятно 
    переобучение, если данных недостаточно (проклятие размерности, экспоненциальное увеличение количества 
    требуемых данных). Так же можно Для выборки составляется матрица объекты-признаки и вектор ответов. На их 
    основе происходит обучение. Решение задачи минимизации функционала ошибки может быть выполнено аналитически с 
    помощью записи матричного уравнения и вычисления обратной матрица; либо оптимизационным методом, те 
    нахождением антиградиента.
--
Регуляризация:
    В общем случае регуляризацией является метод, который штрафует функцию за те или иные ее значения, тем самым 
    корректирую задачу ее оптимизации.
    В машинном обучении при обучении слишком сложных моделей является одним из методов борьбы с переобучением модели наряду с увеличением 
    количества данных и отбором признаков. Штрафует модель за слишком большие по модулю веса, которые характерны переобучению, тем самым
    балансируя ее.
    Иначе говоря, минимизируемая функция ошибки начинает определенным образом зависеть не только от произведения весов на признаки, но и от 
    непосредственного значения весов.
    Метод наименьших квадратов дает оценки, которые имеют нулевое смещение. Регуляризация позволяет получить смещенные оценки с меньшим 
    матожиданием, за счет того, что у этой оценки будет меньше дисперсия.
--
Ridge&Lasso регрессия:
    L2-регуляризатор (Ridge) использует константный коэффициент и квадрат веса. Зависимость весов от признаков остается линейной.
    L1- регуляризатор (Lasso) использует абсолютные значения весов. Обладает свойством отбора признаков. Создает область размером с 
    выбранный коэффициент, в которой веса обнуляются. Оказывается, что таким образом обнуляются признаки наименее значимых признаков 
    (признаков с наименьшей предсказательеой способностью.)
    В байесовской статистике гребневая регрессия соответствует заданию нормального априорного распределения

на коэффициенты линейной модели, а метод лассо — заданию Лапласовского априорного распределения.
--
Проклятие размерности:
    Пусть X — вектор в признаковом пространстве из N бинарных признаков, например:
    X = (0, 0, 1, 0, 1, 1, ..., 1).
    Всего в этом пространстве 2**N различных векторов, а значит размер обучающей выборки, необходимый, чтобы 
    покрыть все возможные комбинации 
    этих признаков будет также порядка 2**N. Другими словами, количество необходимых данных с ростом размерности 
    пространства экспоненциально увеличивается.
--
Дерево решений:
    Дерево решейний обладает такими свойствами как сильный разброс и низкое смещение, что позволяет использовать его в качестве
    базового алгоритма при построении ансамблей моделей. В каждой внутренней вершине записано условие, а в каждом листе дерева — прогноз. Строго говоря, не обязательно решающее дерево должно быть бинарным, но как правило используются именно бинарные. Каждая вершина разбивается по признаку, путем сравнения его на каждом объекте с порогом t. Условия во внутренних вершинах выбираются крайне простыми — проверить, лежит ли значение некоторого признака xj левее, чем заданный порог t.  Можно показать, что значений параметра t столько, сколько различных значений признака xj на обучающей выборке. Строит кусочно-постоянную разделяющую поверхность (при классификации). 
    Линейные модели способны восстанавливать только линейные зависимости, а, следовательно, в случае нелинейных задач, которых подавляющее большинство, смещение при использовании таких алгоритмов будетбольшим.Разброс,наоборот,будетмаленькимиз-замалогочислапараметров,сравнимогосколичеством признаков.Врядлипараметрылинейноймоделисильнопоменяютсяпринезначительномизмененииобучающейвыборки.Решающиедеревья—полнаяпротивоположность.Онихарактеризуютсянизкимсмещением,то есть способны восстанавливать сложные закономерности, и большим разбросом: решающие деревья сильно меняются даже при небольших изменениях обучающей выборки.
--
Критерий разбиения в деревьях решений:
    Строится по принципу разбиения модели в такой вершине, при которой информативность двух разбитых частей будет минимальна (иными словами что бы они были максимально однородны). В случае регресси критерий информативности это дисперсия, а в случае классификации критерий джинни или энтропийний критерий. Строятся деревья жадно, критерием останова обычно выступает минимальное количество объектов в вершине, при построении ансамблей же зачастую используется более грубое ограничение на глубину. Наиболее эфективной является стрижка деревьев, но так же и наиболее ресурсоемкой. При использовании категариальных признаков вершина делится на количество признаков по минимальному суммарному критерию информативности по всем вершинам. Далее при выборе вершины для деления сравнивается критерий по категориальным признакам и минимальный критерий по всем остальным признакам. Деление происходит по минимальному.
--
Нарисовать как будет решать та или иная модель задачу классификации или регрессии:
Линейная модель рисует прямую линию или линейную гиперплоскость. В случае применения спрямляющих пространств (переход к высшему пространству) результатом в исходном пространстве будет нелинейная гиперплоскость.
Деревья и случайный лес строит кусочно-постоянную плоскость.
Метрические алгоритмы так же рисуют нелинейные плоскости, которые строятся на основе изменения расстояний между объектами.
--
Ансамбль моделей:
    Алгоритм, который возвращает среднее или знак среднего по нескольким обученным алгоритмам называется композицией алгоритмов.
    Построение ансамбля это построение N алгоритмов и усреднение ответов по ним, в задаче регрессии это и будет ответом, в задаче классификации необходимо взять знак от получившегося ответа.
    Базовые алгоритмы должны быть различными, иначе в них не будет смысла. Один из способ этого добиться обучать алгоритмы на разных подвыборках обучающей выборки. А поскольку решающие деревья сильно меняются даже от небольших изменений обучающей выборки, такая рандомизация значительно повышает различность базовых алгоритмов. Один из способов - бутстрап. При нем из выборки длины l выбирают с возвращением l объектов. Можно показать, что в бутстрапированной выборке будет содержаться в среднем 63% различных объектов исходной выборки.
    Можно показать что разброс композиции равен равен разбросу одного базового алгоритма деленного на их количество плюс корреляция между ними. Таким образом возникает необходимость уменьшения корреляции между базовыми алгоритмами. Можно использовать бэггинг (случайные подвыборки)
--
Random Forest:
    Это построение леса на бутстрапированных выборках с выбором признаков для разбиения при постоении деревьев из рандомного множества (нового для каждого разбиения). Деревья в случайном лесе часто обучаются до конца, так как случайный лес не переобучается.
    Каждоедерево изслучайного лесаобучаетсянабутстрапированной выборке,в которуюпопадают приблизительно 63% объектов полной выборки. Таким образом, около 37% объектов выборки не использовались при обучении этого дерева, а значит их можно использовать для оценки обобщающей способности случайного леса. Такой подход носит название out-of-bag и позволяет оценивать качество леса без использования отложенной выборки или кросс-валидации. 
--
SVM. Метод опорных векторов:
    Максимизирует зазор между классами и штрафует за нарушение этого зазора. Линейный классификатор с кусочно-
    линейной ф-й потерь и L2-регуляризатором.
    Kernel trick: в качестве скалярного произведения можно использовать линейное ядро (подходит для классификации 
    текстов), полиномиальное ядро, радиальное ядро (подвержено проклятию размерности из-за использования 
    евклидова расстояния)
--
Gradient boosting trees:
1. Строится простой базовый алгоритм, наприемер, который выдает в качестве ответа среднее всех объектов
2. вычисляется вектор ошибок по формуле производной от функции наименьших квадратов для регрессии или логистической регресси для классификации. Получившиеся значения умножаются на некоторый шаг (чем меньше шаг тем больше точность, но медленне сходимость)
3. Строится дерево так, что бы приблизить вектор ошибок с учетом того, что глубина дерева ограничена.
4. Получившийся приближенный вектор ошибок прибавляется к предыдущему к вектору ответов.
5. Операция повторяется до достижения требуемого объема композиции или отсутствия уменьшения ошибки.
--
Понятия ошибки первого и второго рода:
 Ошибка первого рода False positive - отвержение верной нулевой гипотезы, глобально принятие неверного ответа
 Ошибка второго рода False negative - принятие неверной нулевой гипотезы, глобально - отвержение правильного ответа.
 ОПР критичнее, поэтому вероятность ее ограничивается сверху достигаемым уровнем значимости обычно принимаемым 0.05.
 ОВР связана с мощностью - вероятностью отвергнуть неверную нулевую гипотезу. Чтобы найти идеальный критерий для проверки пары нулевая гипотеза – альтернатива, нужно среди всех корректных критериев выбрать критерий с максимальной мощностью. 
--
Валидация моделей:
    2-1 Проблема переобучения
    3.4 Выбор гиперпараметров и сравнение алгоритмов 
    Отложенная выборка. Валидицию модели можно производить на отложенной выборке. Плюс в том, что обучать модель придется один раз, минус в том, что модель может не дообучиться, например если отложенная выборка будет содержать некоторый небольшой подкласс. Что бы решить эту проблему можно построить n разбиений и для каждого померить качество, но так как разбиение происходит случайно и в этом случае нет гарантий, что некоторый особый объект попадет на обучение.
    Кросс валидация. Более системный подход. Выборка делится на k блоков примерно одинакового размера. Далее по очереди каждый из этих блоков используется в качестве тестового, а все остальные - в качестве обучающей выборки. По завершению будут получены k показателе качества. В результате усреднениия получается оценка качества по кросс-валидации. k выбирается тем больше, чем больше выборка. Обычно это 3, 5, 10.
    Часто данные в файле записаны в отсортированном виде по какому-нибудь признаку.Поэтому всегда следует перемешивать выборку прежде,чем производить кросс-валидацию.В ином случае алгоритм будет показывать плохое качество и причина этого будет не так очевидна.
    
    Посмотреть видос по практике в пайтон в конце третьей недели четвертого курса.